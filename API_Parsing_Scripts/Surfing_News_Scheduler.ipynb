{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Author: Akshay Ijantkar\n",
    "### Team: Aqua Wizards\n",
    "### Project: Surfers Bible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://launchschool.com/books/sql/read/table_relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0 1 * * * /usr/bin/python3 /home/ubuntu/pop_db_sch_ss/Daily_Scheduler_Swell_Pollution_Astro_News_API.py >> /home/ubuntu/pop_db_sch_ss/log_Daily_Scheduler_Swell_Pollution_Astro_News_API.txt 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aksha\\Anaconda3\\lib\\site-packages\\statsmodels\\tools\\_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "# import nltk\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib as plt\n",
    "from matplotlib import pyplot\n",
    "# import seaborn as sns; sns.set()\n",
    "# from scipy.stats import norm \n",
    "import matplotlib.pyplot as plt\n",
    "# For Linear regression\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "# For split given dataset into train and test set.\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# To verify models using this metrics \n",
    "# from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# import statsmodels.formula.api as smf\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "# from sklearn import metrics\n",
    "# v\n",
    "from matplotlib import rcParams\n",
    "rcParams['figure.figsize'] = 50,50\n",
    "import pandas_profiling\n",
    "pd.set_option('display.max_rows', 1500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "from pandas import ExcelWriter\n",
    "from pandas import ExcelFile\n",
    "\n",
    "from pygeocoder import Geocoder\n",
    "\n",
    "import sys\n",
    "# from weather_au import api\n",
    "# from weather_au import summary\n",
    "# from weather import place, observations, uv_index\n",
    "import time\n",
    "\n",
    "import json\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "# from catboost import CatBoostClassifier\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# from sklearn.metrics import r2_score\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "# from keras.wrappers.scikit_learn import KerasRegressor\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "# from sklearn.model_selection import KFold\n",
    "# from sklearn.model_selection import cross_validate\n",
    "\n",
    "# import catboost as ctb\n",
    "# from catboost import CatBoostRegressor, FeaturesData, Pool\n",
    "# from scipy.stats import uniform as sp_randFloat\n",
    "# from scipy.stats import randint as sp_randInt\n",
    "# from scipy.stats import uniform\n",
    "# from sklearn.model_selection import RandomizedSearchCV\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# from  sklearn.metrics.pairwise import euclidean_distances\n",
    "# from sklearn.metrics.pairwise import manhattan_distances\n",
    "# from sklearn.metrics.pairwise import pairwise_distances\n",
    "import re\n",
    "# import pprint\n",
    "from datetime import date\n",
    "import datetime\n",
    "# import sqlite3\n",
    "# from sqlite3 import Error\n",
    "from datetime import datetime\n",
    "from dateutil import tz\n",
    "import datetime\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Give Date:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "today_date =  2020-06-01\n",
      "given_date =  2020-06-04\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import datetime\n",
    "no_days_from_today = 3\n",
    "\n",
    "select_date = \"\"\n",
    "\n",
    "if select_date == \"\":    \n",
    "    today = date.today()\n",
    "    today_date = today.strftime(\"%Y-%m-%d\") \n",
    "    given_date =  str((datetime.datetime.strptime(today_date, \"%Y-%m-%d\") + datetime.timedelta(days = no_days_from_today)).date())\n",
    "    print(\"today_date = \", today_date)\n",
    "    print(\"given_date = \", given_date)\n",
    "else:\n",
    "    given_date = select_date\n",
    "    print(\"given_date = \", given_date)\n",
    "# print(\"today_date = \", today_date)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Everything Endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>news_topic</th>\n",
       "      <th>source</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>url</th>\n",
       "      <th>urlToImage</th>\n",
       "      <th>publishedAt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [date, news_topic, source, author, title, description, url, urlToImage, publishedAt]\n",
       "Index: []"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_col_lst = ['date','news_topic','source', 'author', 'title', 'description', 'url', 'urlToImage', 'publishedAt']\n",
    "\n",
    "news_df = pd.DataFrame(columns = news_col_lst)\n",
    "news_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def cleanhtml(raw_html):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', raw_html)\n",
    "    return cleantext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>astronomy_id</th>\n",
       "      <th>date</th>\n",
       "      <th>news_topic</th>\n",
       "      <th>source</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>url</th>\n",
       "      <th>urltoimage</th>\n",
       "      <th>publishedat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2020-05-15</td>\n",
       "      <td>shark+australia</td>\n",
       "      <td>Boingboing.net</td>\n",
       "      <td>Thom Dunn</td>\n",
       "      <td>Giant string-like creature composed of \"millio...</td>\n",
       "      <td>I'm currently re-discovering Jeff Van Der Meer...</td>\n",
       "      <td>https://boingboing.net/2020/04/16/giant-string...</td>\n",
       "      <td>https://i0.wp.com/media.boingboing.net/wp-cont...</td>\n",
       "      <td>2020-04-17T04:23:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2020-05-15</td>\n",
       "      <td>shark+australia</td>\n",
       "      <td>Thepointsguy.com</td>\n",
       "      <td>Ben Smithson</td>\n",
       "      <td>8 common misconceptions about visiting Australia</td>\n",
       "      <td>I spent the first 32 years of my life living i...</td>\n",
       "      <td>http://thepointsguy.com/guide/common-misconcep...</td>\n",
       "      <td>https://i2.wp.com/thepointsguy.com/wp-content/...</td>\n",
       "      <td>2020-04-27T04:30:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2020-05-15</td>\n",
       "      <td>shark+australia</td>\n",
       "      <td>Kron4.com</td>\n",
       "      <td>The Associated Press</td>\n",
       "      <td>Surfer killed in Santa Cruz County shark attac...</td>\n",
       "      <td>&lt;ol&gt;&lt;li&gt;Surfer killed in Santa Cruz County sha...</td>\n",
       "      <td>https://www.kron4.com/news/bay-area/surfer-kil...</td>\n",
       "      <td>https://www.kron4.com/wp-content/uploads/sites...</td>\n",
       "      <td>2020-05-11T10:28:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2020-05-15</td>\n",
       "      <td>shark+australia</td>\n",
       "      <td>Youtube.com</td>\n",
       "      <td>None</td>\n",
       "      <td>Surfer Ben Kelly Dies in Shark Attack - Inside...</td>\n",
       "      <td>&lt;ol&gt;&lt;li&gt;Surfer Ben Kelly Dies in Shark Attack ...</td>\n",
       "      <td>https://www.youtube.com/watch?v=egFyRMmLGjk</td>\n",
       "      <td>https://i.ytimg.com/vi/egFyRMmLGjk/maxresdefau...</td>\n",
       "      <td>2020-05-12T09:01:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2020-05-15</td>\n",
       "      <td>shark+australia</td>\n",
       "      <td>Forbes.com</td>\n",
       "      <td>Melissa Cristina Márquez, Contributor, Melissa...</td>\n",
       "      <td>Missing: South African Great White Sharks. The...</td>\n",
       "      <td>Could Australian demand be driving many coasta...</td>\n",
       "      <td>https://www.forbes.com/sites/melissacristinama...</td>\n",
       "      <td>https://thumbor.forbes.com/thumbor/fit-in/1200...</td>\n",
       "      <td>2020-05-04T17:01:04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   astronomy_id        date       news_topic            source                                             author                                              title                                        description                                                url                                         urltoimage          publishedat\n",
       "0             1  2020-05-15  shark+australia    Boingboing.net                                          Thom Dunn  Giant string-like creature composed of \"millio...  I'm currently re-discovering Jeff Van Der Meer...  https://boingboing.net/2020/04/16/giant-string...  https://i0.wp.com/media.boingboing.net/wp-cont...  2020-04-17T04:23:58\n",
       "1             2  2020-05-15  shark+australia  Thepointsguy.com                                       Ben Smithson   8 common misconceptions about visiting Australia  I spent the first 32 years of my life living i...  http://thepointsguy.com/guide/common-misconcep...  https://i2.wp.com/thepointsguy.com/wp-content/...  2020-04-27T04:30:55\n",
       "2             3  2020-05-15  shark+australia         Kron4.com                               The Associated Press  Surfer killed in Santa Cruz County shark attac...  <ol><li>Surfer killed in Santa Cruz County sha...  https://www.kron4.com/news/bay-area/surfer-kil...  https://www.kron4.com/wp-content/uploads/sites...  2020-05-11T10:28:00\n",
       "3             4  2020-05-15  shark+australia       Youtube.com                                               None  Surfer Ben Kelly Dies in Shark Attack - Inside...  <ol><li>Surfer Ben Kelly Dies in Shark Attack ...        https://www.youtube.com/watch?v=egFyRMmLGjk  https://i.ytimg.com/vi/egFyRMmLGjk/maxresdefau...  2020-05-12T09:01:14\n",
       "4             5  2020-05-15  shark+australia        Forbes.com  Melissa Cristina Márquez, Contributor, Melissa...  Missing: South African Great White Sharks. The...  Could Australian demand be driving many coasta...  https://www.forbes.com/sites/melissacristinama...  https://thumbor.forbes.com/thumbor/fit-in/1200...  2020-05-04T17:01:04"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import psycopg2 as ps\n",
    "\n",
    "# define credentials \n",
    "credentials = {'POSTGRES_ADDRESS' : 'test-surfers-bible-instance.cljoljhkgpfb.ap-southeast-2.rds.amazonaws.com', # change to your endpoint\n",
    "               'POSTGRES_PORT' : 5432, # change to your port\n",
    "               'POSTGRES_USERNAME' : 'ai_postgres', # change to your username\n",
    "               'POSTGRES_PASSWORD' : 'postgres2309', # change to your password\n",
    "               'POSTGRES_DBNAME' : 'test_surfers_bible_db'} # change to your db name\n",
    "\n",
    "# create connection and cursor    \n",
    "conn = ps.connect(host=credentials['POSTGRES_ADDRESS'],\n",
    "                  database=credentials['POSTGRES_DBNAME'],\n",
    "                  user=credentials['POSTGRES_USERNAME'],\n",
    "                  password=credentials['POSTGRES_PASSWORD'],\n",
    "                  port=credentials['POSTGRES_PORT'])\n",
    "\n",
    "cur = conn.cursor()\n",
    "\n",
    "# cur.execute(\"DROP TABLE SEA_WATER_QUALITY_TABLE;\")\n",
    "\n",
    "\n",
    "NEWS_TABLE_df = pd.read_sql(\"SELECT *  FROM NEWS_TABLE;\", conn)\n",
    "\n",
    "cur.close()\n",
    "conn.close()\n",
    "NEWS_TABLE_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_lst = [\n",
    "'shark+australia',\n",
    "'beach+jellyfish+australia',\n",
    "'surfing+events',\n",
    "'beach+rip+currents',\n",
    "'beach+surfing+australia',\n",
    "'surfing+guide',\n",
    "]\n",
    "\n",
    "topics_keywords_lst = [\n",
    "\"shark+australia\", \n",
    "\"beach+jellyfish+australia\", \n",
    "\"surfing+events+competition+Margaret+River+Pro+Rip+Curl+Pro+Australian+Surf+Life+Saving+Championships+Surfest+Quiksilver+Pro\",\n",
    "\"beach+rip+currents+ripcurrents+drowning\", \n",
    "\"beach+surfing+life+australia+weather+waves\", \n",
    "\"surfingaustralia+guide+tutorial\",\n",
    "]\n",
    "for topics_keywords, topic in zip(topics_keywords_lst[:], topics_lst[:]): \n",
    "    news_row_dict = {}\n",
    "    \n",
    "    q_str = qInTitle_str = topics_keywords\n",
    "    NEWS_API_KEY = \"a4d8af5e60fc43cdb556a8d04cf4eeda\"\n",
    "\n",
    "    from_to_date = today_date\n",
    "\n",
    "    get_request_url_str = \"https://newsapi.org/v2/everything?\"\n",
    "\n",
    "    get_request_url_str += \"q=\"\n",
    "    get_request_url_str += q_str\n",
    "\n",
    "    get_request_url_str += \"&qInTitle\"\n",
    "    get_request_url_str += qInTitle_str\n",
    "\n",
    "    get_request_url_str += \"&sortBy=\"\n",
    "    get_request_url_str += \"relevancy\"\n",
    "\n",
    "    get_request_url_str += \"&from\"\n",
    "    get_request_url_str += from_to_date\n",
    "\n",
    "\n",
    "    get_request_url_str += \"&to\"\n",
    "    get_request_url_str += from_to_date\n",
    "\n",
    "    get_request_url_str += \"&language\"\n",
    "    get_request_url_str += \"en\"\n",
    "\n",
    "    get_request_url_str += \"&country\"\n",
    "    get_request_url_str += \"au\"\n",
    "\n",
    "    get_request_url_str += \"&apiKey=\"\n",
    "    get_request_url_str += NEWS_API_KEY\n",
    "    \n",
    "\n",
    "    try:\n",
    "        response_dict = json.loads(requests.get(get_request_url_str).text)\n",
    "\n",
    "        if response_dict['status'] == 'ok':\n",
    "\n",
    "            if response_dict['totalResults'] > 0:\n",
    "#                 print(\"len response_dict['articles'] = \", len(response_dict['articles']))\n",
    "#                 print(\"response_dict['articles'] = \", response_dict['articles'])\n",
    "\n",
    "\n",
    "                for article_dict in response_dict['articles']:\n",
    "                    if article_dict['title'] not in NEWS_TABLE_df.title.values.tolist():\n",
    "                        news_row_dict = {}\n",
    "\n",
    "                        news_row_dict['date'] = today_date\n",
    "                        news_row_dict['news_topic'] = topic\n",
    "\n",
    "                        news_row_dict['source'] = article_dict['source']['name']\n",
    "                        news_row_dict['author'] = article_dict['author']\n",
    "                        news_row_dict['title'] = article_dict['title']\n",
    "                        news_row_dict['description'] = cleanhtml(article_dict['description']) \n",
    "                        news_row_dict['url'] = article_dict['url']\n",
    "                        news_row_dict['urlToImage'] = article_dict['urlToImage']\n",
    "                        news_row_dict['publishedAt'] = article_dict['publishedAt']\n",
    "\n",
    "                        news_df = news_df.append(news_row_dict, ignore_index=True)\n",
    "\n",
    "        else:\n",
    "            print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "            print(\"Status failed!!\")\n",
    "            print(\"topic = \", topic)\n",
    "            print(\"date = \", given_date)\n",
    "            print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "\n",
    "    except:\n",
    "        print(\"###############################################################################\")\n",
    "        print(\"Request failed !!!\")\n",
    "        print(\"topic = \", topic)\n",
    "        print(\"date = \", given_date)\n",
    "        print(\"###############################################################################\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert UTC to AEST TIME of PublishedAt column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_datetime_in_dif_timezones(from_datetime_str, from_timezone_str, to_timezone_str, \n",
    "                                      datetime_format = '%Y-%m-%dT%H:%M:%S'):\n",
    "#     USE pytz.all_timezones to get all timestamps\n",
    "    from datetime import datetime\n",
    "    import pytz\n",
    "    date_time_obj = datetime.strptime(from_datetime_str, datetime_format)\n",
    "#     print(\"date_time_obj = \", date_time_obj)\n",
    "    \n",
    "    old_timezone = pytz.timezone(from_timezone_str)\n",
    "    new_timezone = pytz.timezone(to_timezone_str)\n",
    "    \n",
    "    new_timezone_timestamp = old_timezone.localize(date_time_obj).astimezone(new_timezone).strftime(\"%Y-%m-%dT%H:%M:%S\") \n",
    "#     print(\"new_timezone_timestamp\", new_timezone_timestamp)\n",
    "    return str(new_timezone_timestamp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df[\"publishedAt\"] = news_df['publishedAt'].apply(lambda x: convert_datetime_in_dif_timezones(\n",
    "                                                                            from_datetime_str = x, \n",
    "                                                                            from_timezone_str = 'UTC', \n",
    "                                                                            to_timezone_str ='Australia/Melbourne', \n",
    "                                                                            datetime_format = '%Y-%m-%dT%H:%M:%SZ'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATE NEWS_TABLE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2 as ps\n",
    "\n",
    "# define credentials \n",
    "credentials = {'POSTGRES_ADDRESS' : 'test-surfers-bible-instance.cljoljhkgpfb.ap-southeast-2.rds.amazonaws.com', # change to your endpoint\n",
    "               'POSTGRES_PORT' : 5432, # change to your port\n",
    "               'POSTGRES_USERNAME' : 'ai_postgres', # change to your username\n",
    "               'POSTGRES_PASSWORD' : 'postgres2309', # change to your password\n",
    "               'POSTGRES_DBNAME' : 'test_surfers_bible_db'} # change to your db name\n",
    "\n",
    "# create connection and cursor    \n",
    "conn = ps.connect(host=credentials['POSTGRES_ADDRESS'],\n",
    "                  database=credentials['POSTGRES_DBNAME'],\n",
    "                  user=credentials['POSTGRES_USERNAME'],\n",
    "                  password=credentials['POSTGRES_PASSWORD'],\n",
    "                  port=credentials['POSTGRES_PORT'])\n",
    "\n",
    "cur = conn.cursor()\n",
    "\n",
    "# cur.execute(\"DROP TABLE SEA_WATER_QUALITY_TABLE;\")\n",
    "\n",
    "create_table_query = '''\n",
    "      CREATE TABLE IF NOT EXISTS NEWS_TABLE\n",
    "      (\n",
    "        astronomy_id SERIAL PRIMARY KEY,\n",
    "        date DATE,\n",
    "        news_topic TEXT,\n",
    "        source TEXT, \n",
    "        author TEXT, \n",
    "        title TEXT, \n",
    "        description TEXT, \n",
    "        url TEXT, \n",
    "        urlToImage TEXT, \n",
    "        publishedAt TEXT\n",
    "       ); \n",
    "       '''\n",
    "\n",
    "cur.execute(create_table_query)\n",
    "conn.commit()\n",
    "\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INSERT NEWS_TABLE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# Wall time: 12min 51s\n",
    "import psycopg2 as ps\n",
    "\n",
    "# define credentials \n",
    "credentials = {'POSTGRES_ADDRESS' : 'test-surfers-bible-instance.cljoljhkgpfb.ap-southeast-2.rds.amazonaws.com', # change to your endpoint\n",
    "               'POSTGRES_PORT' : 5432, # change to your port\n",
    "               'POSTGRES_USERNAME' : 'ai_postgres', # change to your username\n",
    "               'POSTGRES_PASSWORD' : 'postgres2309', # change to your password\n",
    "               'POSTGRES_DBNAME' : 'test_surfers_bible_db'} # change to your db name\n",
    "\n",
    "# create connection and cursor    \n",
    "conn = ps.connect(host=credentials['POSTGRES_ADDRESS'],\n",
    "                  database=credentials['POSTGRES_DBNAME'],\n",
    "                  user=credentials['POSTGRES_USERNAME'],\n",
    "                  password=credentials['POSTGRES_PASSWORD'],\n",
    "                  port=credentials['POSTGRES_PORT'])\n",
    "\n",
    "cur = conn.cursor()\n",
    "\n",
    "fill_question_mark_str = str(tuple([\"%s\"  for i in news_df.columns.tolist()])).replace(\"'\", \"\")\n",
    "fill_question_mark_str\n",
    "\n",
    "for row in news_df.itertuples():\n",
    "    data_tuple = tuple(row[1:])\n",
    "\n",
    "#     print(\"data_tuple = \", data_tuple)\n",
    "#     print(\" \")\n",
    "    \n",
    "    cur.execute(\"\"\"\n",
    "                        INSERT INTO NEWS_TABLE\n",
    "                        (\n",
    "                        date,\n",
    "                        news_topic,\n",
    "                        source, \n",
    "                        author, \n",
    "                        title, \n",
    "                        description, \n",
    "                        url, \n",
    "                        urlToImage, \n",
    "                        publishedAt\n",
    "                         ) VALUES  \n",
    "                         \"\"\" + fill_question_mark_str + \" ;\"\n",
    "                , data_tuple)    \n",
    "\n",
    "conn.commit()\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "___\n",
    "___\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
