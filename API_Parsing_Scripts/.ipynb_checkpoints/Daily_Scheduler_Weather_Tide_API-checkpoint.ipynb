{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Author: Akshay Ijantkar\n",
    "### Team: Aqua Wizards\n",
    "### Project: Surfers Bible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://launchschool.com/books/sql/read/table_relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CRONTAB EXPRESSIONA and STATEMENT:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0 2 * * * /usr/bin/python3 /home/ubuntu/pop_db_sch_ss/Daily_Scheduler_Weather_Tide_API.py >> /home/ubuntu/pop_db_sch_ss/log_Daily_Scheduler_Weather_Tide_API.txt 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST API KEY:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"message\":\"You are not subscribed to this API.\"}\n"
     ]
    }
   ],
   "source": [
    "# import requests\n",
    "\n",
    "# url = \"https://tides.p.rapidapi.com/tides\"\n",
    "\n",
    "# querystring = {\"interval\":\"60\",\"duration\":\"1440\",\"latitude\":\"44.414\",\"longitude\":\"-2.097\"}\n",
    "\n",
    "# headers = {\n",
    "#     'x-rapidapi-host': \"tides.p.rapidapi.com\",\n",
    "#     'x-rapidapi-key': \"\"\n",
    "#     }\n",
    "\n",
    "# response = requests.request(\"GET\", url, headers=headers, params=querystring)\n",
    "\n",
    "# print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traceback (most recent call last):\n",
    "#   File \"/home/ubuntu/pop_db_sch_ss/Daily_Scheduler_Weather_Tide_API.py\", line 1377, in <module>\n",
    "#     model.load_model('catboost_model_rand_search_tide_weather_shark_feat')\n",
    "#   File \"/home/ubuntu/.local/lib/python3.6/site-packages/catboost/core.py\", line 2589, in load_model\n",
    "#     self._load_model(fname, format)\n",
    "#   File \"/home/ubuntu/.local/lib/python3.6/site-packages/catboost/core.py\", line 1315, in _load_model\n",
    "#     self._object._load_model(model_file, format)\n",
    "#   File \"_catboost.pyx\", line 4658, in _catboost._CatBoost._load_model\n",
    "#   File \"_catboost.pyx\", line 4661, in _catboost._CatBoost._load_model\n",
    "# _catboost.CatBoostError: catboost/libs/model/model_import_interface.h:19: Model file doesn't exist: catboost_model_rand_search_tide_weather_shark_feat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# log_Daily_Scheduler_Weather_Tide_API\n",
    "wavedirection\n",
    "waveheight\n",
    "waveperiod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib as plt\n",
    "from matplotlib import pyplot\n",
    "# import seaborn as sns; sns.set()\n",
    "# from scipy.stats import norm \n",
    "import matplotlib.pyplot as plt\n",
    "# For Linear regression\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "# For split given dataset into train and test set.\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# To verify models using this metrics \n",
    "# from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# import statsmodels.formula.api as smf\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "# v\n",
    "from matplotlib import rcParams\n",
    "# rcParams['figure.figsize'] = 50,50\n",
    "# import pandas_profiling\n",
    "# pd.set_option('display.max_rows', 1500)\n",
    "# pd.set_option('display.max_columns', 500)\n",
    "# pd.set_option('display.width', 1000)\n",
    "from pandas import ExcelWriter\n",
    "from pandas import ExcelFile\n",
    "\n",
    "# from pygeocoder import Geocoder\n",
    "\n",
    "import sys\n",
    "# from weather_au import api\n",
    "# from weather_au import summary\n",
    "# from weather import place, observations, uv_index\n",
    "import time\n",
    "\n",
    "import json\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "from catboost import CatBoostClassifier\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# from sklearn.metrics import r2_score\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "# from keras.wrappers.scikit_learn import KerasRegressor\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "# from sklearn.model_selection import KFold\n",
    "# from sklearn.model_selection import cross_validate\n",
    "\n",
    "import catboost as ctb\n",
    "# from catboost import CatBoostRegressor, FeaturesData, Pool\n",
    "# from scipy.stats import uniform as sp_randFloat\n",
    "# from scipy.stats import randint as sp_randInt\n",
    "# from scipy.stats import uniform\n",
    "# from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# from  sklearn.metrics.pairwise import euclidean_distances\n",
    "# from sklearn.metrics.pairwise import manhattan_distances\n",
    "# from sklearn.metrics.pairwise import pairwise_distances\n",
    "import re\n",
    "import pprint\n",
    "from datetime import date\n",
    "import datetime\n",
    "# import sqlite3\n",
    "# from sqlite3 import Error\n",
    "from datetime import datetime\n",
    "from dateutil import tz\n",
    "\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decide Date:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "given_date =  2020-06-04\n",
      "given_date_timestamp_epoch =  1591192800\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import datetime\n",
    "no_days_from_today = 3\n",
    "\n",
    "select_date = \"\"\n",
    "\n",
    "if select_date == \"\":    \n",
    "    today = date.today()\n",
    "    today_date = today.strftime(\"%Y-%m-%d\") \n",
    "    given_date =  str((datetime.datetime.strptime(today_date, \"%Y-%m-%d\") + datetime.timedelta(days = no_days_from_today)).date())\n",
    "    given_date_timestamp_epoch = int(time.mktime(time.strptime(given_date, '%Y-%m-%d')))\n",
    "    print(\"today_date = \", today_date)\n",
    "    print(\"given_date = \", given_date)\n",
    "    print(\"given_date_timestamp_epoch = \",given_date_timestamp_epoch)\n",
    "else:\n",
    "    given_date = select_date\n",
    "    given_date_timestamp_epoch = int(time.mktime(time.strptime(given_date, '%Y-%m-%d')))\n",
    "    \n",
    "    print(\"given_date = \", given_date)\n",
    "    print(\"given_date_timestamp_epoch = \",given_date_timestamp_epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2020-06-04', '2020-06-05', '2020-06-06', '2020-06-07', '2020-06-08']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_5days_lst = []\n",
    "for day in range(5):\n",
    "    date_5days_lst.append(str((datetime.datetime.strptime(given_date, \"%Y-%m-%d\") + datetime.timedelta(days = day)).date()))\n",
    "date_5days_lst    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1591192800, 1591279200, 1591365600, 1591452000, 1591538400]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timestamp_epochs_5days_lst = []\n",
    "for date in date_5days_lst:\n",
    "#     + 3600\n",
    "    timestamp_epochs_5days_lst.append(int(time.mktime(time.strptime(date, '%Y-%m-%d'))) )\n",
    "timestamp_epochs_5days_lst    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# READ BEACH DATABASE FROM RDS POSTGRES DB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2 as ps\n",
    "\n",
    "# define credentials \n",
    "credentials = {'POSTGRES_ADDRESS' : 'test-surfers-bible-instance.cljoljhkgpfb.ap-southeast-2.rds.amazonaws.com', # change to your endpoint\n",
    "               'POSTGRES_PORT' : 5432, # change to your port\n",
    "               'POSTGRES_USERNAME' : '', # change to your username\n",
    "               'POSTGRES_PASSWORD' : '', # change to your password\n",
    "               'POSTGRES_DBNAME' : 'test_surfers_bible_db'} # change to your db name\n",
    "\n",
    "# create connection and cursor    \n",
    "conn = ps.connect(host=credentials['POSTGRES_ADDRESS'],\n",
    "                  database=credentials['POSTGRES_DBNAME'],\n",
    "                  user=credentials['POSTGRES_USERNAME'],\n",
    "                  password=credentials['POSTGRES_PASSWORD'],\n",
    "                  port=credentials['POSTGRES_PORT'])\n",
    "\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Beach Table to DF: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "beach_df = pd.read_sql_query(\"SELECT * FROM BEACH_TABLE;\", conn)\n",
    "# beach_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# beach_df.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weather API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_beach_req_col_lst = ['date',\n",
    " 'beach_id',\n",
    " 'beach_name',\n",
    " 'beach_latitude',\n",
    " 'beach_longitude',\n",
    " 'beach_state',\n",
    " 'time',\n",
    " 'summary',\n",
    " 'icon',\n",
    " 'precipIntensity',\n",
    " 'precipProbability',\n",
    " 'temperature',\n",
    " 'apparentTemperature',\n",
    " 'dewPoint',\n",
    " 'humidity',\n",
    " 'pressure',\n",
    " 'windSpeed',\n",
    " 'windGust',\n",
    " 'windBearing',\n",
    " 'cloudCover',\n",
    " 'uvIndex',\n",
    " 'visibility',\n",
    " 'ozone']\n",
    "\n",
    "weather_df = pd.DataFrame(columns = weather_beach_req_col_lst\n",
    "                         )\n",
    "# weather_df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2020-06-04'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "given_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert API response JSON dict to DF row:  Weather API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# Wall time: 24min 24s\n",
    "API_KEY = \"\"\n",
    "cnt = 0\n",
    "# https://api.darksky.net/forecast//-33.869,151.209,2019-12-30T12:00:00\n",
    "\n",
    "# for date in date_5days_lst[:]:\n",
    "    \n",
    "#     timestamp_epochs = int(time.mktime(time.strptime(date, '%Y-%m-%d')))\n",
    "\n",
    "for index, row in beach_df.loc[:,].iterrows():\n",
    "\n",
    "    print(\"beach_name = \", row[\"beach_name\"])\n",
    "    print(\"date = \",given_date,\"\\n\")\n",
    "    print(\"index = \", index)\n",
    "\n",
    "    get_request = \"\"\n",
    "    get_request += \"https://api.darksky.net/forecast/\"\n",
    "    get_request += API_KEY\n",
    "    get_request += \"/\"\n",
    "    get_request += str(row[\"beach_latitude\"])\n",
    "    get_request += \",\"\n",
    "    get_request += str(row[\"beach_longitude\"])\n",
    "    get_request += \",\"\n",
    "    get_request += str(given_date_timestamp_epoch)\n",
    "\n",
    "    response_dict = json.loads(requests.get(get_request).text)\n",
    "\n",
    "    print(\"get_request = \", get_request)\n",
    "    print(\"****************************************************************************************************\")\n",
    "#     print(\"response_dict = \",response_dict)\n",
    "\n",
    "    if len(list(response_dict.keys())) > 2:\n",
    "        cnt = cnt + 1\n",
    "        print(\"cnt =>>>>>>>>>>>>>>>>>>>>> \", cnt)\n",
    "\n",
    "        weather_row_dict = {}\n",
    "\n",
    "        for per_hr_attri_dict in response_dict['hourly']['data']:\n",
    "\n",
    "            weather_row_dict[\"date\"] = given_date\n",
    "\n",
    "            for col in ['beach_id', 'beach_name', 'beach_latitude', 'beach_longitude', 'beach_state']:\n",
    "                weather_row_dict[col] = row[col]\n",
    "\n",
    "            for hr_attri in per_hr_attri_dict.keys():\n",
    "                weather_row_dict[hr_attri] = per_hr_attri_dict[hr_attri]                    \n",
    "\n",
    "            weather_row_dict[\"nearest-station\"] = response_dict['flags']['nearest-station']\n",
    "            weather_row_dict[\"sources\"] = response_dict['flags']['sources']\n",
    "            weather_row_dict[\"offset\"] = response_dict['offset']            \n",
    "\n",
    "            weather_df = weather_df.append(weather_row_dict, ignore_index=True)\n",
    "\n",
    "\n",
    "    else:\n",
    "        print(\"############################################################################################\")\n",
    "        print(\"Response Failed...!\")\n",
    "        print(\"beach_name = \", row[\"beach_name\"])\n",
    "        print(\"date = \",date,\"\\n\")\n",
    "        print(\"index = \", index)\n",
    "        print(\"############################################################################################\")\n",
    "#     time.sleep(1)\n",
    "# weather_df            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert timestamp epochs to datetime format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df[\"datetime\"] = weather_df[\"time\"].apply(lambda x: time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://launchschool.com/books/sql/read/table_relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Weather Table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2 as ps\n",
    "\n",
    "# define credentials \n",
    "credentials = {'POSTGRES_ADDRESS' : 'test-surfers-bible-instance.cljoljhkgpfb.ap-southeast-2.rds.amazonaws.com', # change to your endpoint\n",
    "               'POSTGRES_PORT' : 5432, # change to your port\n",
    "               'POSTGRES_USERNAME' : 'ai_postgres', # change to your username\n",
    "               'POSTGRES_PASSWORD' : 'postgres2309', # change to your password\n",
    "               'POSTGRES_DBNAME' : 'test_surfers_bible_db'} # change to your db name\n",
    "\n",
    "# create connection and cursor    \n",
    "conn = ps.connect(host=credentials['POSTGRES_ADDRESS'],\n",
    "                  database=credentials['POSTGRES_DBNAME'],\n",
    "                  user=credentials['POSTGRES_USERNAME'],\n",
    "                  password=credentials['POSTGRES_PASSWORD'],\n",
    "                  port=credentials['POSTGRES_PORT'])\n",
    "\n",
    "cur = conn.cursor()\n",
    "\n",
    "create_table_query = '''\n",
    "      CREATE TABLE IF NOT EXISTS WEATHER_TABLE\n",
    "      (\n",
    "        weather_id SERIAL PRIMARY KEY,\n",
    "        date DATE,\n",
    "        beach_id INTEGER NOT NULL,\n",
    "        beach_name TEXT,\n",
    "        beach_latitude REAL,\n",
    "        beach_longitude REAL,\n",
    "        beach_state TEXT,\n",
    "        time REAL,\n",
    "        summary TEXT,\n",
    "        icon TEXT,\n",
    "        precipIntensity REAL,\n",
    "        precipProbability REAL,\n",
    "        temperature REAL,\n",
    "        apparentTemperature REAL,\n",
    "        dewPoint REAL,\n",
    "        humidity REAL,\n",
    "        pressure REAL,\n",
    "        windSpeed REAL,\n",
    "        windGust REAL,\n",
    "        windBearing REAL,\n",
    "        cloudCover REAL,\n",
    "        uvIndex REAL,\n",
    "        visibility REAL,\n",
    "        ozone REAL,\n",
    "        nearest_station REAL,\n",
    "        time_offset REAL,\n",
    "        precipType TEXT,\n",
    "        sources TEXT,\n",
    "        datetime TEXT,\n",
    "        FOREIGN KEY (beach_id) REFERENCES beach_table(beach_id) ON DELETE CASCADE\n",
    "       ); \n",
    "       '''\n",
    "\n",
    "cur.execute(create_table_query)\n",
    "conn.commit()\n",
    "\n",
    "cur.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insert rows in Table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# Wall time: 12min 16s\n",
    "import psycopg2 as ps\n",
    "\n",
    "# define credentials \n",
    "credentials = {'POSTGRES_ADDRESS' : 'test-surfers-bible-instance.cljoljhkgpfb.ap-southeast-2.rds.amazonaws.com', # change to your endpoint\n",
    "               'POSTGRES_PORT' : 5432, # change to your port\n",
    "               'POSTGRES_USERNAME' : '', # change to your username\n",
    "               'POSTGRES_PASSWORD' : '', # change to your password\n",
    "               'POSTGRES_DBNAME' : 'test_surfers_bible_db'} # change to your db name\n",
    "\n",
    "# create connection and cursor    \n",
    "conn = ps.connect(host=credentials['POSTGRES_ADDRESS'],\n",
    "                  database=credentials['POSTGRES_DBNAME'],\n",
    "                  user=credentials['POSTGRES_USERNAME'],\n",
    "                  password=credentials['POSTGRES_PASSWORD'],\n",
    "                  port=credentials['POSTGRES_PORT'])\n",
    "\n",
    "cur = conn.cursor()\n",
    "\n",
    "fill_question_mark_str = str(tuple([\"%s\"  for i in weather_df.columns.tolist()])).replace(\"'\", \"\")\n",
    "fill_question_mark_str\n",
    "\n",
    "for row in weather_df.itertuples():\n",
    "    data_tuple = tuple(row[1:])\n",
    "\n",
    "#     print(\"data_tuple = \", data_tuple)\n",
    "#     print(\" \")\n",
    "    \n",
    "    cur.execute(\"\"\"\n",
    "                        INSERT INTO WEATHER_TABLE\n",
    "                        (\n",
    "                            date,\n",
    "                            beach_id,\n",
    "                            beach_name,\n",
    "                            beach_latitude,\n",
    "                            beach_longitude,\n",
    "                            beach_state,\n",
    "                            time,\n",
    "                            summary,\n",
    "                            icon,\n",
    "                            precipIntensity,\n",
    "                            precipProbability,\n",
    "                            temperature,\n",
    "                            apparentTemperature,\n",
    "                            dewPoint,\n",
    "                            humidity,\n",
    "                            pressure,\n",
    "                            windSpeed,\n",
    "                            windGust,\n",
    "                            windBearing,\n",
    "                            cloudCover,\n",
    "                            uvIndex,\n",
    "                            visibility,\n",
    "                            ozone,\n",
    "                            nearest_station,\n",
    "                            time_offset,\n",
    "                            precipType,\n",
    "                            sources,\n",
    "                            datetime\n",
    "                         ) VALUES  \n",
    "                         \"\"\" + fill_question_mark_str + \" ;\"\n",
    "                , data_tuple)    \n",
    "\n",
    "conn.commit()\n",
    "\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df[\"date_beach_name\"] = weather_df[\n",
    "                                                ['date', 'beach_name']\n",
    "                                             ].apply(lambda x: '|'.join(x.astype(str).values), axis=1)\n",
    "# weather_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_dataset_path = \"D:\\Monash_University_Stuff\\Final_Semester\\IE\\Surfers_Bible_Code_Commit\\Datasets\\Daily_weather_tide_log\\\\\"\n",
    "# weather_df.to_csv(log_dataset_path + \"weather_df\"+\"_10_05\"+\".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AGGREGATE WEATHER:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_ops_lst = ['max','min','mean','median','std','var','sem']\n",
    "weather_numeric_features = [\"temperature\", \"apparentTemperature\", \"dewPoint\", \"humidity\", \"windSpeed\", \"windBearing\", \"uvIndex\",\n",
    "                    \"cloudCover\"]\n",
    "\n",
    "agg_dict = dict(zip(weather_numeric_features, [agg_ops_lst for i in range(len(weather_numeric_features))]))\n",
    "# agg_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in weather_numeric_features:\n",
    "    weather_df[col] = weather_df[col].astype(float) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_beach_req_col_lst = [\n",
    "    \"date_beach_name\",\n",
    "    'date',\n",
    "     'beach_name',\n",
    "     'beach_latitude',\n",
    "     'beach_longitude',\n",
    "     'beach_state',\n",
    "]\n",
    "\n",
    "agg_weather_df = weather_df[weather_numeric_features + [\"date_beach_name\"]].groupby('date_beach_name').agg(agg_dict)\n",
    "\n",
    "agg_weather_df.columns = [\"_\".join(x) for x in agg_weather_df.columns.ravel()]\n",
    "agg_weather_df.reset_index(level=0, inplace=True)\n",
    "agg_weather_df =  pd.merge(\n",
    "                           agg_weather_df, \n",
    "                           weather_df[weather_beach_req_col_lst],\n",
    "                           left_on = \"date_beach_name\",\n",
    "                           right_on = \"date_beach_name\",\n",
    "                           how = \"inner\"\n",
    "                            )\n",
    "\n",
    "agg_weather_df.drop_duplicates(inplace = True)\n",
    "agg_weather_df.reset_index(inplace = True, \n",
    "                           drop = True)\n",
    "\n",
    "print(\"agg_weather_df.shape = \", agg_weather_df.shape)\n",
    "# agg_weather_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# agg_weather_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_dataset_path = \"D:\\Monash_University_Stuff\\Final_Semester\\IE\\Surfers_Bible_Code_Commit\\Datasets\\Daily_weather_tide_log\\\\\"\n",
    "# agg_weather_df.to_csv(log_dataset_path + \"agg_weather_df\"+\"_10_05\"+\".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END OF WEATHER API PROCESSING:\n",
    "\n",
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TIDE API PROCESSING STARTS:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Tide related DF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heights_tide_df = pd.DataFrame(columns = [\n",
    "                                         'date',\n",
    "                                        'beach_id',\n",
    "                                        'beach_name',\n",
    "                                        'beach_latitude',\n",
    "                                        'beach_longitude',\n",
    "                                        'beach_state',\n",
    "                                         'timestamp', \n",
    "                                         'datetime', \n",
    "                                         'height', \n",
    "                                         'state', \n",
    "                                         'origin_distance',\n",
    "                                         \"origin_distance_unit\", \n",
    "                                         \"origin_latitude\", \n",
    "                                         \"origin_longitude\", \n",
    "                                         \"timezone\"\n",
    "                                        ]\n",
    "                              )\n",
    "\n",
    "# heights_tide_df \n",
    "extremes_tide_df = pd.DataFrame(columns = [\n",
    "                                        'date',\n",
    "                                        'beach_id',\n",
    "                                        'beach_name',\n",
    "                                        'beach_latitude',\n",
    "                                        'beach_longitude',\n",
    "                                        'beach_state',   \n",
    "                                        'timestamp', \n",
    "                                        'datetime', \n",
    "                                        'height', \n",
    "                                        'state'\n",
    "                                          ]\n",
    "                               )\n",
    "\n",
    "# extremes_tide_df\n",
    "datum_tide_df = pd.DataFrame(columns = [\n",
    "                                        'date',\n",
    "                                        'beach_id',\n",
    "                                        'beach_name',\n",
    "                                        'beach_latitude',\n",
    "                                        'beach_longitude',\n",
    "                                        'beach_state',                \n",
    "                                        'timestamp', \n",
    "                                        'datetime', \n",
    "                                        'datum', \n",
    "                                        'LAT', \n",
    "                                        'HAT'\n",
    "                                        ]\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert API response JSON dict to DF row:  TIDE API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# Wall time: 14min 4s\n",
    "# datum_tide_df\n",
    "cnt = 0\n",
    "for index, row in beach_df.loc[:,:].iterrows():\n",
    "#     if index >= 0:\n",
    "    print(\"beach_name = \", row[\"beach_name\"])\n",
    "    print(\"index = \", index)\n",
    "    timestamp_epochs = int(time.mktime(time.strptime(given_date, '%Y-%m-%d')))\n",
    "\n",
    "    url = \"https://tides.p.rapidapi.com/tides\"    \n",
    "    querystring = {\"interval\":\"60\",\n",
    "                   \"duration\":\"1440\",\n",
    "#                    \"duration\": \"7200\",\n",
    "                   \"timestamp\": str(timestamp_epochs),\n",
    "                   \"latitude\":str(row[\"beach_latitude\"]),\n",
    "                   \"longitude\":str(row[\"beach_longitude\"])}\n",
    "\n",
    "    headers = {\n",
    "        'x-rapidapi-host': \"tides.p.rapidapi.com\",\n",
    "        'x-rapidapi-key': \"\",\n",
    "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36'\n",
    "        }\n",
    "\n",
    "    response_dict = json.loads(requests.request(\"GET\", url, headers = headers, params = querystring).text)\n",
    "#     print(\"response_dict - \", response_dict)\n",
    "    print(\" \")\n",
    "    if response_dict['status'] == 200:\n",
    "        cnt = cnt + 1\n",
    "        print(\"cnt = \", cnt)\n",
    "\n",
    "#         Updating heights_tide_df \n",
    "#                                          'date',\n",
    "#                                         'beach_id',\n",
    "#                                         'beach_name',\n",
    "#                                         'beach_latitude',\n",
    "#                                         'beach_longitude',\n",
    "#                                         'beach_state',  \n",
    "#                                          'timestamp', \n",
    "#                                          'datetime', \n",
    "#                                          'height', \n",
    "#                                          'state', \n",
    "#                                          'origin_distance',\n",
    "#                                          \"origin_distance_unit\", \n",
    "#                                          \"origin_latitude\", \n",
    "#                                          \"origin_longitude\", \n",
    "#                                          \"timezone\"\n",
    "\n",
    "        heights_tide_row_dict = {}\n",
    "        heights_tide_row_dict[\"date\"] = given_date\n",
    "        heights_tide_row_dict[\"beach_id\"] = row[\"beach_id\"]\n",
    "        heights_tide_row_dict[\"beach_name\"] = row[\"beach_name\"]\n",
    "        heights_tide_row_dict[\"beach_latitude\"] = row[\"beach_latitude\"]\n",
    "        heights_tide_row_dict[\"beach_longitude\"] = row[\"beach_longitude\"]\n",
    "        heights_tide_row_dict[\"beach_state\"] = row[\"beach_state\"]\n",
    "        \n",
    "\n",
    "        heights_tide_row_dict[\"origin_distance\"] = response_dict['origin']['distance']\n",
    "        heights_tide_row_dict[\"origin_distance_unit\"] = response_dict['origin']['unit']\n",
    "        heights_tide_row_dict[\"origin_latitude\"] = response_dict['origin']['latitude']\n",
    "        heights_tide_row_dict[\"origin_longitude\"] = response_dict['origin']['longitude']\n",
    "        heights_tide_row_dict[\"timezone\"] = response_dict[\"timezone\"]\n",
    "\n",
    "        for ht_dict in response_dict['heights']:\n",
    "            heights_tide_row_dict[\"timestamp\"] = ht_dict['timestamp']\n",
    "            heights_tide_row_dict[\"datetime\"] = ht_dict['datetime']\n",
    "            heights_tide_row_dict[\"height\"] = ht_dict['height']\n",
    "            heights_tide_row_dict[\"state\"] = ht_dict['state']\n",
    "\n",
    "            heights_tide_df = heights_tide_df.append(heights_tide_row_dict, \n",
    "                                                 ignore_index = True)\n",
    "\n",
    "#         Updating extremes_tide_df:\n",
    "#                                         'date',\n",
    "#                                         'beach_address',\n",
    "#                                         'beach_name',\n",
    "#                                         'country_state',\n",
    "#                                         'country',\n",
    "#                                         'latitude',\n",
    "#                                         'longitude',    \n",
    "#                                         'timestamp', \n",
    "#                                         'datetime', \n",
    "#                                         'height', \n",
    "#                                         'state'\n",
    "\n",
    "        extremes_tide_row_dict = {}\n",
    "        extremes_tide_row_dict[\"date\"] = given_date\n",
    "        extremes_tide_row_dict[\"beach_id\"] = row[\"beach_id\"]\n",
    "        extremes_tide_row_dict[\"beach_name\"] = row[\"beach_name\"]\n",
    "        extremes_tide_row_dict[\"beach_latitude\"] = row[\"beach_latitude\"]\n",
    "        extremes_tide_row_dict[\"beach_longitude\"] = row[\"beach_longitude\"]\n",
    "        extremes_tide_row_dict[\"beach_state\"] = row[\"beach_state\"] \n",
    "\n",
    "        for extremes_dict in response_dict['extremes']:\n",
    "\n",
    "            extremes_tide_row_dict[\"timestamp\"] = extremes_dict['timestamp']\n",
    "            extremes_tide_row_dict[\"datetime\"] = extremes_dict['datetime']\n",
    "            extremes_tide_row_dict[\"height\"] = extremes_dict['height']\n",
    "            extremes_tide_row_dict[\"state\"] = extremes_dict['state']   \n",
    "\n",
    "            extremes_tide_df = extremes_tide_df.append(extremes_tide_row_dict, \n",
    "                                                 ignore_index = True)\n",
    "\n",
    "        #         Updating datum_tide_df\n",
    "#                                         'date',\n",
    "#                                         'beach_address',\n",
    "#                                         'beach_name',\n",
    "#                                         'country_state',\n",
    "#                                         'country',\n",
    "#                                         'latitude',\n",
    "#                                         'longitude',                \n",
    "#                                         'timestamp', \n",
    "#                                         'datetime', \n",
    "#                                         'datum', \n",
    "#                                         'LAT', \n",
    "#                                         'HAT'\n",
    "        datum_tide_row_dict = {}\n",
    "        datum_tide_row_dict[\"date\"] = given_date\n",
    "        datum_tide_row_dict[\"beach_id\"] = row[\"beach_id\"]\n",
    "        datum_tide_row_dict[\"beach_name\"] = row[\"beach_name\"]\n",
    "        datum_tide_row_dict[\"beach_latitude\"] = row[\"beach_latitude\"]\n",
    "        datum_tide_row_dict[\"beach_longitude\"] = row[\"beach_longitude\"]\n",
    "        datum_tide_row_dict[\"beach_state\"] = row[\"beach_state\"] \n",
    "\n",
    "        datum_tide_row_dict[\"timestamp\"] = response_dict[\"timestamp\"]\n",
    "        datum_tide_row_dict[\"datetime\"] = response_dict[\"datetime\"]\n",
    "        datum_tide_row_dict[\"datum\"] = response_dict[\"datum\"]\n",
    "        datum_tide_row_dict[\"LAT\"] = response_dict['datums'][\"LAT\"]\n",
    "        datum_tide_row_dict[\"HAT\"] = response_dict['datums'][\"HAT\"]\n",
    "\n",
    "        datum_tide_df = datum_tide_df.append(\n",
    "                                            datum_tide_row_dict,\n",
    "                                            ignore_index = True\n",
    "                                            )\n",
    "        print(\" \")\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"heights_tide_df.shape = \",heights_tide_df.shape)\n",
    "# heights_tide_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_dataset_path = \"D:\\Monash_University_Stuff\\Final_Semester\\IE\\Surfers_Bible_Code_Commit\\Datasets\\Daily_weather_tide_log\\\\\"\n",
    "# heights_tide_df.to_csv(log_dataset_path + \"heights_tide_df\"+\"_10_05\"+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"extremes_tide_df.shape = \", extremes_tide_df.shape)\n",
    "# extremes_tide_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_dataset_path = \"D:\\Monash_University_Stuff\\Final_Semester\\IE\\Surfers_Bible_Code_Commit\\Datasets\\Daily_weather_tide_log\\\\\"\n",
    "# extremes_tide_df.to_csv(log_dataset_path + \"extremes_tide_df\"+\"_10_05\"+\".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONVERT UTC TIME to LOCAL TIME:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_datetime_in_dif_timezones(from_datetime_str, from_timezone_str, to_timezone_str, \n",
    "                                      datetime_format = '%Y-%m-%dT%H:%M:%S'):\n",
    "#     USE pytz.all_timezones to get all timestamps\n",
    "    from datetime import datetime\n",
    "    import pytz\n",
    "    date_time_obj = datetime.strptime(from_datetime_str, datetime_format)\n",
    "#     print(\"date_time_obj = \", date_time_obj)\n",
    "    \n",
    "    old_timezone = pytz.timezone(from_timezone_str)\n",
    "    new_timezone = pytz.timezone(to_timezone_str)\n",
    "    \n",
    "    new_timezone_timestamp = old_timezone.localize(date_time_obj).astimezone(new_timezone).strftime(\"%Y-%m-%dT%H:%M:%S\") \n",
    "#     print(\"new_timezone_timestamp\", new_timezone_timestamp)\n",
    "    return str(new_timezone_timestamp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heights_tide_df[\"datetime\"] = heights_tide_df[\"datetime\"].apply(lambda x: convert_datetime_in_dif_timezones(from_datetime_str = x, \n",
    "                                                                                  from_timezone_str = 'UTC', \n",
    "                                                                                  to_timezone_str ='Australia/Melbourne', \n",
    "                                                                                datetime_format = '%Y-%m-%dT%H:%M:%S+00:00'))\n",
    "\n",
    "extremes_tide_df[\"datetime\"] = extremes_tide_df[\"datetime\"].apply(lambda x: convert_datetime_in_dif_timezones(from_datetime_str = x, \n",
    "                                                                                  from_timezone_str = 'UTC', \n",
    "                                                                                  to_timezone_str ='Australia/Melbourne', \n",
    "                                                                                datetime_format = '%Y-%m-%dT%H:%M:%S+00:00'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get date from Date time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heights_tide_df['date'] = heights_tide_df['datetime'].apply(lambda x: x.split(\"T\")[0])\n",
    "extremes_tide_df['date'] = extremes_tide_df['datetime'].apply(lambda x: x.split(\"T\")[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create date_coordinates column which will be used as PK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heights_tide_df[\"date_beach_name\"] = heights_tide_df[\n",
    "                                                     ['date', 'beach_name']\n",
    "                                                     ].apply(lambda x: '|'.join(x.astype(str).values), axis=1)\n",
    "# heights_tide_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extremes_tide_df[\"date_beach_name\"] = extremes_tide_df[\n",
    "                                                     ['date', 'beach_name']\n",
    "                                                     ].apply(lambda x: '|'.join(x.astype(str).values), axis=1)\n",
    "# extremes_tide_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATE TIDE_HEIGHT_24HR_TABLE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define credentials \n",
    "credentials = {'POSTGRES_ADDRESS' : 'test-surfers-bible-instance.cljoljhkgpfb.ap-southeast-2.rds.amazonaws.com', # change to your endpoint\n",
    "               'POSTGRES_PORT' : 5432, # change to your port\n",
    "               'POSTGRES_USERNAME' : '', # change to your username\n",
    "               'POSTGRES_PASSWORD' : '', # change to your password\n",
    "               'POSTGRES_DBNAME' : 'test_surfers_bible_db'} # change to your db name\n",
    "\n",
    "# create connection and cursor    \n",
    "conn = ps.connect(host=credentials['POSTGRES_ADDRESS'],\n",
    "                  database=credentials['POSTGRES_DBNAME'],\n",
    "                  user=credentials['POSTGRES_USERNAME'],\n",
    "                  password=credentials['POSTGRES_PASSWORD'],\n",
    "                  port=credentials['POSTGRES_PORT'])\n",
    "\n",
    "cur = conn.cursor()\n",
    "\n",
    "create_table_query = \"\"\" \n",
    "                            CREATE TABLE IF NOT EXISTS TIDE_HEIGHT_TABLE (\n",
    "                                tide_height_id SERIAL PRIMARY KEY ,\n",
    "                                date DATE,\n",
    "                                beach_id INTEGER NOT NULL,\n",
    "                                beach_name TEXT,\n",
    "                                beach_latitude REAL,\n",
    "                                beach_longitude REAL,\n",
    "                                beach_state TEXT,\n",
    "                                timestamp INTEGER,\n",
    "                                datetime TEXT,\n",
    "                                height REAL,\n",
    "                                state TEXT,\n",
    "                                origin_distance REAL,\n",
    "                                origin_distance_unit TEXT,\n",
    "                                origin_latitude REAL,\n",
    "                                origin_longitude REAL,\n",
    "                                timezone TEXT,\n",
    "                                date_beach_name TEXT,\n",
    "                                FOREIGN KEY (beach_id) REFERENCES beach_table(beach_id) ON DELETE CASCADE\n",
    "                                ); \n",
    "                            \"\"\"\n",
    "\n",
    "cur.execute(create_table_query)\n",
    "conn.commit()\n",
    "\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INSERTION TIDE_HEIGHT_24HR_TABLE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# Wall time: 12min 51s\n",
    "import psycopg2 as ps\n",
    "\n",
    "# define credentials \n",
    "credentials = {'POSTGRES_ADDRESS' : 'test-surfers-bible-instance.cljoljhkgpfb.ap-southeast-2.rds.amazonaws.com', # change to your endpoint\n",
    "               'POSTGRES_PORT' : 5432, # change to your port\n",
    "               'POSTGRES_USERNAME' : '', # change to your username\n",
    "               'POSTGRES_PASSWORD' : '', # change to your password\n",
    "               'POSTGRES_DBNAME' : 'test_surfers_bible_db'} # change to your db name\n",
    "\n",
    "# create connection and cursor    \n",
    "conn = ps.connect(host=credentials['POSTGRES_ADDRESS'],\n",
    "                  database=credentials['POSTGRES_DBNAME'],\n",
    "                  user=credentials['POSTGRES_USERNAME'],\n",
    "                  password=credentials['POSTGRES_PASSWORD'],\n",
    "                  port=credentials['POSTGRES_PORT'])\n",
    "\n",
    "cur = conn.cursor()\n",
    "\n",
    "fill_question_mark_str = str(tuple([\"%s\"  for i in heights_tide_df.columns.tolist()])).replace(\"'\", \"\")\n",
    "fill_question_mark_str\n",
    "\n",
    "for row in heights_tide_df.itertuples():\n",
    "    data_tuple = tuple(row[1:])\n",
    "\n",
    "#     print(\"data_tuple = \", data_tuple)\n",
    "#     print(\" \")\n",
    "    \n",
    "    cur.execute(\"\"\"\n",
    "                        INSERT INTO TIDE_HEIGHT_TABLE\n",
    "                        (\n",
    "                                date,\n",
    "                                beach_id,\n",
    "                                beach_name,\n",
    "                                beach_latitude,\n",
    "                                beach_longitude,\n",
    "                                beach_state,\n",
    "                                timestamp,\n",
    "                                datetime,\n",
    "                                height,\n",
    "                                state,\n",
    "                                origin_distance,\n",
    "                                origin_distance_unit,\n",
    "                                origin_latitude,\n",
    "                                origin_longitude,\n",
    "                                timezone,\n",
    "                                date_beach_name\n",
    "                         ) VALUES  \n",
    "                         \"\"\" + fill_question_mark_str + \" ;\"\n",
    "                , data_tuple)    \n",
    "\n",
    "conn.commit()\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATE EXTREMES_HEIGHT_24HR_TABLE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define credentials \n",
    "credentials = {'POSTGRES_ADDRESS' : 'test-surfers-bible-instance.cljoljhkgpfb.ap-southeast-2.rds.amazonaws.com', # change to your endpoint\n",
    "               'POSTGRES_PORT' : 5432, # change to your port\n",
    "               'POSTGRES_USERNAME' : 'ai_postgres', # change to your username\n",
    "               'POSTGRES_PASSWORD' : 'postgres2309', # change to your password\n",
    "               'POSTGRES_DBNAME' : 'test_surfers_bible_db'} # change to your db name\n",
    "\n",
    "# create connection and cursor    \n",
    "conn = ps.connect(host=credentials['POSTGRES_ADDRESS'],\n",
    "                  database=credentials['POSTGRES_DBNAME'],\n",
    "                  user=credentials['POSTGRES_USERNAME'],\n",
    "                  password=credentials['POSTGRES_PASSWORD'],\n",
    "                  port=credentials['POSTGRES_PORT'])\n",
    "\n",
    "cur = conn.cursor()\n",
    "\n",
    "create_table_query = \"\"\" \n",
    "                            CREATE TABLE IF NOT EXISTS EXTREMES_HEIGHT_TABLE (\n",
    "                                extremes_height_id SERIAL PRIMARY KEY ,\n",
    "                                date DATE,\n",
    "                                beach_id INTEGER NOT NULL,\n",
    "                                beach_name TEXT,\n",
    "                                beach_latitude REAL,\n",
    "                                beach_longitude REAL,\n",
    "                                beach_state TEXT,\n",
    "                                timestamp INTEGER,\n",
    "                                datetime TEXT,\n",
    "                                height REAL,\n",
    "                                state TEXT,\n",
    "                                date_beach_name TEXT,\n",
    "                                FOREIGN KEY (beach_id) REFERENCES beach_table(beach_id) ON DELETE CASCADE\n",
    "                                ); \n",
    "                            \"\"\"\n",
    "\n",
    "cur.execute(create_table_query)\n",
    "conn.commit()\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INSERTION EXTREMES_HEIGHT_24HR_TABLE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# Wall time: 12min 51s\n",
    "import psycopg2 as ps\n",
    "\n",
    "# define credentials \n",
    "credentials = {'POSTGRES_ADDRESS' : 'test-surfers-bible-instance.cljoljhkgpfb.ap-southeast-2.rds.amazonaws.com', # change to your endpoint\n",
    "               'POSTGRES_PORT' : 5432, # change to your port\n",
    "               'POSTGRES_USERNAME' : '', # change to your username\n",
    "               'POSTGRES_PASSWORD' : '', # change to your password\n",
    "               'POSTGRES_DBNAME' : 'test_surfers_bible_db'} # change to your db name\n",
    "\n",
    "# create connection and cursor    \n",
    "conn = ps.connect(host=credentials['POSTGRES_ADDRESS'],\n",
    "                  database=credentials['POSTGRES_DBNAME'],\n",
    "                  user=credentials['POSTGRES_USERNAME'],\n",
    "                  password=credentials['POSTGRES_PASSWORD'],\n",
    "                  port=credentials['POSTGRES_PORT'])\n",
    "\n",
    "cur = conn.cursor()\n",
    "\n",
    "fill_question_mark_str = str(tuple([\"%s\"  for i in extremes_tide_df.columns.tolist()])).replace(\"'\", \"\")\n",
    "fill_question_mark_str\n",
    "\n",
    "for row in extremes_tide_df.itertuples():\n",
    "    data_tuple = tuple(row[1:])\n",
    "\n",
    "#     print(\"data_tuple = \", data_tuple)\n",
    "#     print(\" \")\n",
    "    \n",
    "    cur.execute(\"\"\"\n",
    "                        INSERT INTO EXTREMES_HEIGHT_TABLE\n",
    "                        (\n",
    "                            date,\n",
    "                            beach_id,\n",
    "                            beach_name,\n",
    "                            beach_latitude,\n",
    "                            beach_longitude,\n",
    "                            beach_state,\n",
    "                            timestamp,\n",
    "                            datetime,\n",
    "                            height,\n",
    "                            state,\n",
    "                            date_beach_name\n",
    "                         ) VALUES  \n",
    "                         \"\"\" + fill_question_mark_str + \" ;\"\n",
    "                , data_tuple)    \n",
    "\n",
    "conn.commit()\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AGGREGATES TIDES:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "req_beach_loc_attri_lst = [ \n",
    "                            \"date\",\n",
    "                            \"beach_id\",\n",
    "                            \"beach_name\",\n",
    "                            \"beach_latitude\",\n",
    "                            \"beach_longitude\",\n",
    "                            \"beach_state\",\n",
    "                            \"date_beach_name\",\n",
    "                            ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HEIGHTS FALL TIDES AGG:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "agg_ops_lst = ['max','min','mean','median','std','var','sem']\n",
    "numeric_features = [\"height\"]\n",
    "agg_dict = dict(zip(numeric_features, [agg_ops_lst for i in range(len(numeric_features))]))\n",
    "# agg_dict\n",
    "\n",
    "agg_fall_heights_tide_df = heights_tide_df.loc[heights_tide_df['state'] == \"FALLING\",:\n",
    "                                              ].groupby([\n",
    "                                                        'date_beach_name',\n",
    "                                                        ]).agg(agg_dict)\n",
    "\n",
    "\n",
    "agg_fall_heights_tide_df.columns = [\"_fall_\".join(x) for x in agg_fall_heights_tide_df.columns.ravel()]\n",
    "\n",
    "agg_fall_heights_tide_df =  pd.merge(\n",
    "                               agg_fall_heights_tide_df, \n",
    "                               heights_tide_df[req_beach_loc_attri_lst],\n",
    "                               left_on = \"date_beach_name\",\n",
    "                               right_on = \"date_beach_name\",\n",
    "                               how = \"inner\"\n",
    "                                    )\n",
    "\n",
    "agg_fall_heights_tide_df.drop_duplicates(inplace = True)\n",
    "\n",
    "agg_fall_heights_tide_df.reset_index(inplace = True, \n",
    "                                     drop = True)\n",
    "\n",
    "print(\"agg_fall_heights_tide_df.shape = \", agg_fall_heights_tide_df.shape)\n",
    "# agg_fall_heights_tide_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_dataset_path = \"D:\\Monash_University_Stuff\\Final_Semester\\IE\\Surfers_Bible_Code_Commit\\Datasets\\Daily_weather_tide_log\\\\\"\n",
    "# agg_fall_heights_tide_df.to_csv(log_dataset_path + \"agg_fall_heights_tide_df\"+\"_10_05\"+\".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HEIGHTS RISE TIDES AGG:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "agg_ops_lst = ['max','min','mean','median','std','var','sem']\n",
    "numeric_features = [\"height\"]\n",
    "agg_dict = dict(zip(numeric_features, [agg_ops_lst for i in range(len(numeric_features))]))\n",
    "# agg_dict\n",
    "\n",
    "agg_rise_heights_tide_df = heights_tide_df.loc[heights_tide_df['state'] == \"RISING\",:\n",
    "                                              ].groupby([\n",
    "                                                        'date_beach_name',\n",
    "                                                        ]\n",
    "                                                       ).agg(agg_dict)\n",
    "\n",
    "agg_rise_heights_tide_df.columns = [\"_rise_\".join(x) for x in agg_rise_heights_tide_df.columns.ravel()]\n",
    "\n",
    "agg_rise_heights_tide_df =  pd.merge(\n",
    "                               agg_rise_heights_tide_df, \n",
    "                               heights_tide_df[req_beach_loc_attri_lst],\n",
    "                               left_on = \"date_beach_name\",\n",
    "                               right_on = \"date_beach_name\",\n",
    "                               how = \"inner\"\n",
    "                                    )\n",
    "\n",
    "agg_rise_heights_tide_df.drop_duplicates(inplace = True)\n",
    "\n",
    "agg_rise_heights_tide_df.reset_index(inplace = True, \n",
    "                                     drop = True)\n",
    "\n",
    "print(\"agg_rise_heights_tide_df.shape = \", agg_rise_heights_tide_df.shape)\n",
    "# agg_rise_heights_tide_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_dataset_path = \"D:\\Monash_University_Stuff\\Final_Semester\\IE\\Surfers_Bible_Code_Commit\\Datasets\\Daily_weather_tide_log\\\\\"\n",
    "# agg_rise_heights_tide_df.to_csv(log_dataset_path + \"agg_rise_heights_tide_df\"+\"_10_05\"+\".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXTREMES HIGH TIDES AGG:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_ops_lst = ['max','min','mean']\n",
    "numeric_features = [\"height\"]\n",
    "agg_dict = dict(zip(numeric_features, [agg_ops_lst for i in range(len(numeric_features))]))\n",
    "# agg_dict\n",
    "\n",
    "agg_high_tide_extremes_tide_df = extremes_tide_df.loc[extremes_tide_df['state'] == \"HIGH TIDE\",:\n",
    "                                              ].groupby([\n",
    "                                                        'date_beach_name',\n",
    "                                                        ]).agg(agg_dict)\n",
    "\n",
    "agg_high_tide_extremes_tide_df.columns = [\"_high_tide_\".join(x) for x in agg_high_tide_extremes_tide_df.columns.ravel()]\n",
    "\n",
    "agg_high_tide_extremes_tide_df =  pd.merge(\n",
    "                               agg_high_tide_extremes_tide_df, \n",
    "                               extremes_tide_df[req_beach_loc_attri_lst],\n",
    "                               left_on = \"date_beach_name\",\n",
    "                               right_on = \"date_beach_name\",\n",
    "                               how = \"inner\"\n",
    "                                    )\n",
    "\n",
    "agg_high_tide_extremes_tide_df.drop_duplicates(inplace = True)\n",
    "\n",
    "agg_high_tide_extremes_tide_df.reset_index(inplace = True, \n",
    "                                     drop = True)\n",
    "\n",
    "print(\"agg_high_tide_extremes_tide_df.shape = \", agg_high_tide_extremes_tide_df.shape)\n",
    "# agg_high_tide_extremes_tide_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_dataset_path = \"D:\\Monash_University_Stuff\\Final_Semester\\IE\\Surfers_Bible_Code_Commit\\Datasets\\Daily_weather_tide_log\\\\\"\n",
    "# agg_high_tide_extremes_tide_df.to_csv(log_dataset_path + \"agg_high_tide_extremes_tide_df\"+\"_10_05\"+\".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXTREMES LOW TIDES AGG:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_ops_lst = ['max','min','mean']\n",
    "numeric_features = [\"height\"]\n",
    "agg_dict = dict(zip(numeric_features, [agg_ops_lst for i in range(len(numeric_features))]))\n",
    "# agg_dict\n",
    "\n",
    "agg_low_tide_extremes_tide_df = extremes_tide_df.loc[extremes_tide_df['state'] == \"LOW TIDE\",:\n",
    "                                              ].groupby(['date_beach_name']).agg(agg_dict)\n",
    "\n",
    "agg_low_tide_extremes_tide_df.columns = [\"_low_tide_\".join(x) for x in agg_low_tide_extremes_tide_df.columns.ravel()]\n",
    "\n",
    "agg_low_tide_extremes_tide_df =  pd.merge(\n",
    "                               agg_low_tide_extremes_tide_df, \n",
    "                               extremes_tide_df[req_beach_loc_attri_lst],\n",
    "                               left_on = \"date_beach_name\",\n",
    "                               right_on = \"date_beach_name\",\n",
    "                               how = \"inner\"\n",
    "                                    )\n",
    "\n",
    "agg_low_tide_extremes_tide_df.drop_duplicates(inplace = True)\n",
    "\n",
    "agg_low_tide_extremes_tide_df.reset_index(inplace = True, \n",
    "                                     drop = True)\n",
    "\n",
    "print(\"agg_low_tide_extremes_tide_df.shape = \", agg_low_tide_extremes_tide_df.shape)\n",
    "# agg_low_tide_extremes_tide_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_dataset_path = \"D:\\Monash_University_Stuff\\Final_Semester\\IE\\Surfers_Bible_Code_Commit\\Datasets\\Daily_weather_tide_log\\\\\"\n",
    "# agg_low_tide_extremes_tide_df.to_csv(log_dataset_path + \"agg_low_tide_extremes_tide_df\"+\"_10_05\"+\".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END OF TIDE API PROCESSING:\n",
    "\n",
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joing WEATHER AND TIDE AGG DF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agg_weather_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# agg_rise_heights_tide_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tide_df_lst = [\n",
    "                agg_rise_heights_tide_df, agg_fall_heights_tide_df,\n",
    "                agg_high_tide_extremes_tide_df, agg_low_tide_extremes_tide_df,\n",
    "#                 agg_datum_tide_df,\n",
    "              ]\n",
    "join_weather_tide_df = agg_weather_df.copy()\n",
    "for df in tide_df_lst:\n",
    "    join_weather_tide_df = pd.merge(\n",
    "                               join_weather_tide_df, \n",
    "                               df.drop(['date', 'beach_name', 'beach_name', 'beach_state', \n",
    "                                        'beach_latitude','beach_longitude'], axis = 1, inplace = False),\n",
    "                               left_on = \"date_beach_name\",\n",
    "                               right_on = \"date_beach_name\",\n",
    "                               how = \"inner\")\n",
    "# drop_col_lst = []\n",
    "# for col in join_weather_tide_df.columns.tolist():\n",
    "#     if col.endswith(\"_x\")|col.endswith(\"_y\"):\n",
    "#         drop_col_lst.append(col)\n",
    "        \n",
    "# join_weather_tide_df.drop(drop_col_lst, \n",
    "#                           axis=1, \n",
    "#                           inplace=True)\n",
    "\n",
    "        \n",
    "# join_weather_tide_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join_weather_tide_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function: Get month, season, month_day from date: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_month_season_month_day_func(date):\n",
    "    month = date.split(\"-\")[1]\n",
    "    month_day = date.split(\"-\")[2]\n",
    "    if (int(month) == 9) | (int(month) == 10) | (int(month) == 11):\n",
    "        season = \"spring\"\n",
    "    elif (int(month) == 12) | (int(month) == 1) | (int(month) == 2):\n",
    "        season = \"summer\"\n",
    "    elif (int(month) == 3) | (int(month) == 4) | (int(month) == 5):\n",
    "        season = \"autumn\"\n",
    "    elif (int(month) == 6) | (int(month) == 7) | (int(month) == 8):\n",
    "        season = \"winter\"\n",
    "    return month, season, month_day\n",
    "join_weather_tide_df['month'], join_weather_tide_df['season'], join_weather_tide_df['month_day'] = zip(*join_weather_tide_df.apply(lambda x: \n",
    "                                                                                        get_month_season_month_day_func(\n",
    "                                                                                            x['date']), \n",
    "                                                                                        axis = 1))\n",
    "# join_weather_tide_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML MODEL INFERENCE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features_lst = [\n",
    "#     Using Weather features\n",
    "    'temperature_max',\n",
    " 'temperature_min',\n",
    " 'temperature_mean',\n",
    " 'temperature_median',\n",
    " 'temperature_std',\n",
    " 'temperature_var',\n",
    " 'temperature_sem',\n",
    " 'apparentTemperature_max',\n",
    " 'apparentTemperature_min',\n",
    " 'apparentTemperature_mean',\n",
    " 'apparentTemperature_median',\n",
    " 'apparentTemperature_std',\n",
    " 'apparentTemperature_var',\n",
    " 'apparentTemperature_sem',\n",
    " 'dewPoint_max',\n",
    " 'dewPoint_min',\n",
    " 'dewPoint_mean',\n",
    " 'dewPoint_median',\n",
    " 'dewPoint_std',\n",
    " 'dewPoint_var',\n",
    " 'dewPoint_sem',\n",
    " 'humidity_max',\n",
    " 'humidity_min',\n",
    " 'humidity_mean',\n",
    " 'humidity_median',\n",
    " 'humidity_std',\n",
    " 'humidity_var',\n",
    " 'humidity_sem',\n",
    " 'windSpeed_max',\n",
    " 'windSpeed_min',\n",
    " 'windSpeed_mean',\n",
    " 'windSpeed_median',\n",
    " 'windSpeed_std',\n",
    " 'windSpeed_var',\n",
    " 'windSpeed_sem',\n",
    " 'windBearing_max',\n",
    " 'windBearing_min',\n",
    " 'windBearing_mean',\n",
    " 'windBearing_median',\n",
    " 'windBearing_std',\n",
    " 'windBearing_var',\n",
    " 'windBearing_sem',\n",
    " 'uvIndex_max',\n",
    " 'uvIndex_min',\n",
    " 'uvIndex_mean',\n",
    " 'uvIndex_median',\n",
    " 'uvIndex_std',\n",
    " 'uvIndex_var',\n",
    " 'uvIndex_sem',\n",
    " 'cloudCover_max',\n",
    " 'cloudCover_min',\n",
    " 'cloudCover_mean',\n",
    " 'cloudCover_median',\n",
    " 'cloudCover_std',\n",
    " 'cloudCover_var',\n",
    " 'cloudCover_sem',\n",
    "# Tides Features                        \n",
    " 'height_fall_max',\n",
    " 'height_fall_min',\n",
    " 'height_fall_mean',\n",
    " 'height_fall_median',\n",
    " 'height_fall_std',\n",
    " 'height_fall_var',\n",
    " 'height_fall_sem',\n",
    " 'height_rise_max',\n",
    " 'height_rise_min',\n",
    " 'height_rise_mean',\n",
    " 'height_rise_median',\n",
    " 'height_rise_std',\n",
    " 'height_rise_var',\n",
    " 'height_rise_sem',\n",
    " 'height_high_tide_max',\n",
    " 'height_high_tide_min',\n",
    " 'height_high_tide_mean',\n",
    " 'height_low_tide_max',\n",
    " 'height_low_tide_min',\n",
    " 'height_low_tide_mean',\n",
    "#  'LAT_datum_mean',\n",
    "#  'HAT_datum_mean',\n",
    " 'beach_latitude',\n",
    " 'beach_longitude'\n",
    "]\n",
    "cat_features_lst = [\n",
    " 'month',\n",
    " 'month_day',\n",
    " 'season']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHARK ATTACK PREDICTION:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD Catboots Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'loss_function':'Logloss', # objective function\n",
    "          'eval_metric':'Accuracy', # metric\n",
    "          'verbose': 200, # output to stdout info about training process every 200 iterations\n",
    "          'random_seed': 100\n",
    "         }\n",
    "\n",
    "model = CatBoostClassifier(**params)\n",
    "model.load_model('catboost_model_rand_search_tide_weather_shark_feat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_weather_tide_df[\"shark_attack_percentage\"] = 0.0\n",
    "for index, row in join_weather_tide_df[numeric_features_lst+cat_features_lst].iterrows():\n",
    "#     print(model.predict_proba(row.values)[1])\n",
    "    join_weather_tide_df.loc[index, \"shark_attack_percentage\"] = round(float(model.predict_proba(row.values)[1])*100.0, 3)\n",
    "#     join_weather_tide_df.loc[index, \"shark_attack_percentage\"] = model.predict_proba(row.values)[1]\n",
    "\n",
    "# join_weather_tide_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHARK SIGHTING PREDICTION:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load CSV clean addresses with Coordinates CSV as Dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_path = r\"D:\\Monash_University_Stuff\\Final_Semester\\IE\\Surfers_Bible_Code_Commit\\Datasets\\\\\"\n",
    "# dataset_path = r\"\"\n",
    "join_shark_weather_tide_df = pd.read_csv(dataset_path+\"for_ml_join_shark_weather_tide_df.csv\")\n",
    "# raw_shark_df = pd.read_excel(dataset_path + \"shark_file_geolocation_checkpoint.xlsx\")\n",
    "print(\"join_shark_weather_tide_df.shape = \", join_shark_weather_tide_df.shape)\n",
    "join_shark_weather_tide_df.rename(columns = {\n",
    "                                            \"lat\":\"beach_latitude\",\n",
    "                                            \"lng\":\"beach_longitude\"}, inplace = True)\n",
    "# join_shark_weather_tide_df.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Cosine Similarity: Comparing point with point for the percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# Wall time: 2min 36s\n",
    "threshold = 0.90\n",
    "join_weather_tide_df['shark_sighting_percentage'] = 0.0\n",
    "for index_o, row_o in join_weather_tide_df[numeric_features_lst].iterrows():\n",
    "    cos_sim_lst = []\n",
    "    for index_i, row_i in join_shark_weather_tide_df[numeric_features_lst].iterrows():\n",
    "    \n",
    "        cosine_similarity_score = cosine_similarity(\n",
    "                                                    X = row_o.values.reshape(1, -1), \n",
    "                                                    Y = row_i.values.reshape(1, -1), \n",
    "                                                    dense_output=True).flatten()[0]\n",
    "        \n",
    "        cos_sim_lst.append(cosine_similarity_score)\n",
    "    \n",
    "    no_vals_above = len([i for i in cos_sim_lst if i > threshold])\n",
    "    \n",
    "    percent_vals_above = round((no_vals_above/float(join_shark_weather_tide_df.shape[0]))*100.0, 3)\n",
    "    \n",
    "    join_weather_tide_df.loc[index_o, 'shark_sighting_percentage'] = percent_vals_above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function: Convert probability to levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percent_2_level_func(percent):\n",
    "    if (percent > 0) & (percent <= 25):\n",
    "        return \"Low\"\n",
    "    elif (percent > 25) & (percent <= 50):\n",
    "        return \"Moderately Low\"\n",
    "    elif (percent > 50) & (percent <= 75):\n",
    "        return \"Moderately High\"\n",
    "    elif (percent > 75) & (percent <= 100):\n",
    "        return \"High\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_weather_tide_df[\"shark_sighting_level\"] = join_weather_tide_df[\"shark_sighting_percentage\"].apply(percent_2_level_func)\n",
    "join_weather_tide_df[\"shark_attack_level\"] = join_weather_tide_df[\"shark_attack_percentage\"].apply(percent_2_level_func)\n",
    "# join_weather_tide_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_dataset_path = \"D:\\Monash_University_Stuff\\Final_Semester\\IE\\Surfers_Bible_Code_Commit\\Datasets\\Daily_weather_tide_log\\\\\"\n",
    "# join_weather_tide_df.to_csv(log_dataset_path + \"join_weather_tide_df\"+\"_10_05\"+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join_weather_tide_df\n",
    "join_weather_tide_df = join_weather_tide_df.loc[:,~join_weather_tide_df.columns.duplicated()]\n",
    "join_weather_tide_df.drop(['beach_id_y'], axis=1, inplace=True)\n",
    "join_weather_tide_df.rename(columns = {\"beach_id_x\": \"beach_id\"}, inplace =True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATE join_weather_tide_TABLE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define credentials \n",
    "credentials = {'POSTGRES_ADDRESS' : 'test-surfers-bible-instance.cljoljhkgpfb.ap-southeast-2.rds.amazonaws.com', # change to your endpoint\n",
    "               'POSTGRES_PORT' : 5432, # change to your port\n",
    "               'POSTGRES_USERNAME' : '', # change to your username\n",
    "               'POSTGRES_PASSWORD' : '', # change to your password\n",
    "               'POSTGRES_DBNAME' : 'test_surfers_bible_db'} # change to your db name\n",
    "\n",
    "# create connection and cursor    \n",
    "conn = ps.connect(host=credentials['POSTGRES_ADDRESS'],\n",
    "                  database=credentials['POSTGRES_DBNAME'],\n",
    "                  user=credentials['POSTGRES_USERNAME'],\n",
    "                  password=credentials['POSTGRES_PASSWORD'],\n",
    "                  port=credentials['POSTGRES_PORT'])\n",
    "\n",
    "cur = conn.cursor()\n",
    "\n",
    "create_table_query = \"\"\" \n",
    "                            CREATE TABLE IF NOT EXISTS SHARK_PREDICTION_TABLE (\n",
    "                                shark_prediction_id SERIAL PRIMARY KEY ,\n",
    "                                date_beach_name TEXT,\n",
    "                                temperature_max REAL,\n",
    "                                temperature_min REAL,\n",
    "                                temperature_mean REAL,\n",
    "                                temperature_median REAL,\n",
    "                                temperature_std REAL,\n",
    "                                temperature_var REAL,\n",
    "                                temperature_sem REAL,\n",
    "                                apparentTemperature_max REAL,\n",
    "                                apparentTemperature_min REAL,\n",
    "                                apparentTemperature_mean REAL,\n",
    "                                apparentTemperature_median REAL,\n",
    "                                apparentTemperature_std REAL,\n",
    "                                apparentTemperature_var REAL,\n",
    "                                apparentTemperature_sem REAL,\n",
    "                                dewPoint_max REAL,\n",
    "                                dewPoint_min REAL,\n",
    "                                dewPoint_mean REAL,\n",
    "                                dewPoint_median REAL,\n",
    "                                dewPoint_std REAL,\n",
    "                                dewPoint_var REAL,\n",
    "                                dewPoint_sem REAL,\n",
    "                                humidity_max REAL,\n",
    "                                humidity_min REAL,\n",
    "                                humidity_mean REAL,\n",
    "                                humidity_median REAL,\n",
    "                                humidity_std REAL,\n",
    "                                humidity_var REAL,\n",
    "                                humidity_sem REAL,\n",
    "                                windSpeed_max REAL,\n",
    "                                windSpeed_min REAL,\n",
    "                                windSpeed_mean REAL,\n",
    "                                windSpeed_median REAL,\n",
    "                                windSpeed_std REAL, \n",
    "                                windSpeed_var REAL,\n",
    "                                windSpeed_sem REAL,\n",
    "                                windBearing_max REAL,\n",
    "                                windBearing_min REAL,\n",
    "                                windBearing_mean REAL,\n",
    "                                windBearing_median REAL,\n",
    "                                windBearing_std REAL,\n",
    "                                windBearing_var REAL,\n",
    "                                windBearing_sem REAL,\n",
    "                                uvIndex_max REAL,\n",
    "                                uvIndex_min REAL,\n",
    "                                uvIndex_mean REAL,\n",
    "                                uvIndex_median REAL,\n",
    "                                uvIndex_std REAL,\n",
    "                                uvIndex_var REAL,\n",
    "                                uvIndex_sem REAL,\n",
    "                                cloudCover_max REAL,\n",
    "                                cloudCover_min REAL,\n",
    "                                cloudCover_mean REAL,\n",
    "                                cloudCover_median REAL,\n",
    "                                cloudCover_std REAL,\n",
    "                                cloudCover_var REAL,\n",
    "                                cloudCover_sem REAL,\n",
    "                                date DATE,\n",
    "                                beach_name TEXT,\n",
    "                                beach_latitude REAL,\n",
    "                                beach_longitude REAL,\n",
    "                                beach_state TEXT,\n",
    "                                height_rise_max REAL,\n",
    "                                height_rise_min REAL,\n",
    "                                height_rise_mean REAL,\n",
    "                                height_rise_median REAL,\n",
    "                                height_rise_std REAL,\n",
    "                                height_rise_var REAL,\n",
    "                                height_rise_sem REAL,\n",
    "                                beach_id INTEGER,\n",
    "                                height_fall_max REAL,\n",
    "                                height_fall_min REAL,\n",
    "                                height_fall_mean REAL,\n",
    "                                height_fall_median REAL,\n",
    "                                height_fall_std REAL,\n",
    "                                height_fall_var REAL,\n",
    "                                height_fall_sem REAL,\n",
    "                                height_high_tide_max REAL,\n",
    "                                height_high_tide_min REAL,\n",
    "                                height_high_tide_mean REAL,\n",
    "                                height_low_tide_max REAL,\n",
    "                                height_low_tide_min REAL,\n",
    "                                height_low_tide_mean REAL,\n",
    "                                month TEXT,\n",
    "                                season TEXT,\n",
    "                                month_day TEXT,\n",
    "                                shark_attack_percentage REAL,\n",
    "                                shark_sighting_percentage REAL,\n",
    "                                shark_sighting_level TEXT,\n",
    "                                shark_attack_level TEXT,\n",
    "                                FOREIGN KEY (beach_id) REFERENCES beach_table(beach_id) ON DELETE CASCADE\n",
    "                                ); \n",
    "                            \"\"\"\n",
    "\n",
    "cur.execute(create_table_query)\n",
    "conn.commit()\n",
    "\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INSERTION join_weather_tide_TABLE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# Wall time: 34 s\n",
    "import psycopg2 as ps\n",
    "\n",
    "# define credentials \n",
    "credentials = {'POSTGRES_ADDRESS' : 'test-surfers-bible-instance.cljoljhkgpfb.ap-southeast-2.rds.amazonaws.com', # change to your endpoint\n",
    "               'POSTGRES_PORT' : 5432, # change to your port\n",
    "               'POSTGRES_USERNAME' : '', # change to your username\n",
    "               'POSTGRES_PASSWORD' : '', # change to your password\n",
    "               'POSTGRES_DBNAME' : 'test_surfers_bible_db'} # change to your db name\n",
    "\n",
    "# create connection and cursor    \n",
    "conn = ps.connect(host=credentials['POSTGRES_ADDRESS'],\n",
    "                  database=credentials['POSTGRES_DBNAME'],\n",
    "                  user=credentials['POSTGRES_USERNAME'],\n",
    "                  password=credentials['POSTGRES_PASSWORD'],\n",
    "                  port=credentials['POSTGRES_PORT'])\n",
    "\n",
    "cur = conn.cursor()\n",
    "\n",
    "fill_question_mark_str = str(tuple([\"%s\"  for i in join_weather_tide_df.columns.tolist()])).replace(\"'\", \"\")\n",
    "fill_question_mark_str\n",
    "\n",
    "for row in join_weather_tide_df.itertuples():\n",
    "    data_tuple = tuple(row[1:])\n",
    "\n",
    "    print(\"data_tuple = \", data_tuple)\n",
    "    print(\" \")\n",
    "    \n",
    "    cur.execute(\"\"\"\n",
    "                        INSERT INTO SHARK_PREDICTION_TABLE\n",
    "                        (\n",
    "                            date_beach_name,\n",
    "                            temperature_max,\n",
    "                            temperature_min,\n",
    "                            temperature_mean,\n",
    "                            temperature_median,\n",
    "                            temperature_std,\n",
    "                            temperature_var,\n",
    "                            temperature_sem,\n",
    "                            apparentTemperature_max,\n",
    "                            apparentTemperature_min,\n",
    "                            apparentTemperature_mean,\n",
    "                            apparentTemperature_median,\n",
    "                            apparentTemperature_std,\n",
    "                            apparentTemperature_var,\n",
    "                            apparentTemperature_sem,\n",
    "                            dewPoint_max,\n",
    "                            dewPoint_min,\n",
    "                            dewPoint_mean,\n",
    "                            dewPoint_median,\n",
    "                            dewPoint_std,\n",
    "                            dewPoint_var,\n",
    "                            dewPoint_sem,\n",
    "                            humidity_max,\n",
    "                            humidity_min,\n",
    "                            humidity_mean,\n",
    "                            humidity_median,\n",
    "                            humidity_std,\n",
    "                            humidity_var,\n",
    "                            humidity_sem,\n",
    "                            windSpeed_max,\n",
    "                            windSpeed_min,\n",
    "                            windSpeed_mean,\n",
    "                            windSpeed_median,\n",
    "                            windSpeed_std,\n",
    "                            windSpeed_var,\n",
    "                            windSpeed_sem,\n",
    "                            windBearing_max,\n",
    "                            windBearing_min,\n",
    "                            windBearing_mean,\n",
    "                            windBearing_median,\n",
    "                            windBearing_std,\n",
    "                            windBearing_var,\n",
    "                            windBearing_sem,\n",
    "                            uvIndex_max,\n",
    "                            uvIndex_min,\n",
    "                            uvIndex_mean,\n",
    "                            uvIndex_median,\n",
    "                            uvIndex_std,\n",
    "                            uvIndex_var,\n",
    "                            uvIndex_sem,\n",
    "                            cloudCover_max,\n",
    "                            cloudCover_min,\n",
    "                            cloudCover_mean,\n",
    "                            cloudCover_median,\n",
    "                            cloudCover_std,\n",
    "                            cloudCover_var,\n",
    "                            cloudCover_sem,\n",
    "                            date,\n",
    "                            beach_name,\n",
    "                            beach_latitude,\n",
    "                            beach_longitude,\n",
    "                            beach_state,\n",
    "                            height_rise_max,\n",
    "                            height_rise_min,\n",
    "                            height_rise_mean,\n",
    "                            height_rise_median,\n",
    "                            height_rise_std,\n",
    "                            height_rise_var,\n",
    "                            height_rise_sem,\n",
    "                            beach_id,\n",
    "                            height_fall_max,\n",
    "                            height_fall_min,\n",
    "                            height_fall_mean,\n",
    "                            height_fall_median,\n",
    "                            height_fall_std,\n",
    "                            height_fall_var,\n",
    "                            height_fall_sem,\n",
    "                            height_high_tide_max,\n",
    "                            height_high_tide_min,\n",
    "                            height_high_tide_mean,\n",
    "                            height_low_tide_max,\n",
    "                            height_low_tide_min,\n",
    "                            height_low_tide_mean,\n",
    "                            month,\n",
    "                            season,\n",
    "                            month_day,\n",
    "                            shark_attack_percentage,\n",
    "                            shark_sighting_percentage,\n",
    "                            shark_sighting_level,\n",
    "                            shark_attack_level\n",
    "                         ) VALUES  \n",
    "                         \"\"\" + fill_question_mark_str + \" ;\"\n",
    "                , data_tuple)    \n",
    "\n",
    "conn.commit()\n",
    "\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
